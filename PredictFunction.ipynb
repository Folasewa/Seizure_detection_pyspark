{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from pyspark.ml import Pipeline\n",
    "import gcsfs\n",
    "import json\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def process_test_sample(sample, with_latency, transforms): \n",
    "            name, sample_X = sample\n",
    "            data = sample_X['data']\n",
    "            transformed_data = transforms(data)\n",
    "            return (name, transformed_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_subjects(gs_dir, subjects, sc, num_nodes):\n",
    "    \n",
    "    json_str_rdd = sc.textFile(gs_dir + '/SETTINGS.json')\n",
    "    json_str = ''.join(json_str_rdd.collect())\n",
    "    settings = json.loads(json_str)\n",
    "    \n",
    "    proj_name = settings['gcp-project-name']\n",
    "    proj_dir = settings['gcp-bucket-project-dir']\n",
    "    dataset_dir = settings['dataset-dir']\n",
    "    result_dir = settings[\"submission-dir\"]\n",
    "    model_dir = settings[\"data-cache-dir\"]\n",
    "    \n",
    "    fs = gcsfs.GCSFileSystem(project = proj_name)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for subject in subjects:\n",
    "        \n",
    "        #Load data into rdd\n",
    "        start_time = time.time()\n",
    "        loader = dataloader('/'.join([proj_dir,dataset_dir,subject]), fs)\n",
    "        test_raw, test_names = loader.load_test_data()\n",
    "\n",
    "        partitionNum = num_nodes * 10\n",
    "        test_raw_names = list(zip(test_names, test_raw))\n",
    "        end_time = time.time()\n",
    "        print('--- '+ subject + \": Test Data Loading %s seconds ---\" % (end_time - start_time))\n",
    "        #Data preprocessing and transformation\n",
    "        start_time = time.time()\n",
    "        test_rdd = sc.parallelize(test_raw_names, partitionNum)\n",
    "        transformed_test_rdd = test_rdd.map(lambda x: process_test_sample(x, True, sample_transform)).cache()\n",
    "        #transformed_interictal_rdd = interictal_rdd.map(lambda x: process_raw_sample(x, False, sample_transform)).cache()\n",
    "\n",
    "        def rddToDf(x):\n",
    "            '''Convert rdd to  and pass this function in Row() args'''\n",
    "            name, sample_X = x\n",
    "            d = {}\n",
    "            d['clip'] = name\n",
    "            d['features'] = Vectors.dense(sample_X)\n",
    "            return d\n",
    "\n",
    "        test_df = transformed_test_rdd.map(lambda x: Row(**rddToDf(x))).toDF()\n",
    "\n",
    "        test_df.cache()\n",
    "        \n",
    "\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print('--- '+ subject + \": Test Data Transformation %s seconds ---\" % (end_time - start_time))\n",
    "\n",
    "        #Predicting samples with saved models or retrain a new model for prediction if not exists\n",
    "        \n",
    "        model = load_model(gs_dir, subject)\n",
    "        if not model:\n",
    "            model = train_model(gs_dir, [subject], sc, fs, num_nodes)[0]\n",
    "        start_time = time.time()\n",
    "        result = model.transform(test_df)\n",
    "        end_time = time.time()\n",
    "        print('--- '+ subject + \": Making Predictions %s seconds ---\" % (end_time - start_time))\n",
    "        print('--- '+ subject + \": Saving Trained Model ---\" )\n",
    "        results.append(result)\n",
    "\n",
    "\n",
    "    return results\n",
    "        \n",
    "    \n",
    "def generate_pred_result(results, gs_dir, sc):\n",
    "    \n",
    "    def resultRddToDf(x_and_name):\n",
    "        '''Convert rdd to  and pass this function in Row() args'''\n",
    "        name, x = x_and_name\n",
    "        d = {}\n",
    "        d['clip'] = name\n",
    "        d['seizure'] = float(x[1] + x[2])\n",
    "        d['early'] = float(x[2])\n",
    "        return d\n",
    "\n",
    "    submission_df = results[0].select(['clip','probability']).rdd.map(lambda x: Row(**resultRddToDf(x))).toDF()\n",
    "    \n",
    "    for i in range(1, len(results)):\n",
    "        result_prob_df = result_df.select(['clip','probability']).rdd.map(lambda x: Row(**resultRddToDf(x))).toDF()\n",
    "        submission_df = submission_df.unionAll(result_prob_df)\n",
    "    return submission_df\n",
    "    \n",
    "\n",
    "#Main Function for testing \n",
    "num_nodes = 2\n",
    "subjects = ['Patient_8']\n",
    "gs_dir = \"gs://seizure_detection_data/notebooks/seizure_detection_spark_gcp\"\n",
    "\n",
    "appName = 'seizure_detection'\n",
    "conf = SparkConf().setAppName(appName).setMaster('local')\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set(\"spark.executor.instances\", str(2 * num_nodes))\n",
    "        .set('spark.executor.memory', '15G')\n",
    "        .set('spark.driver.memory', '15G')\n",
    "        .set('spark.driver.maxResultSize', '15G'))\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "    \n",
    "json_str_rdd = sc.textFile(gs_dir + '/SETTINGS.json')\n",
    "json_str = ''.join(json_str_rdd.collect())\n",
    "settings = json.loads(json_str)\n",
    "\n",
    "proj_name = settings['gcp-project-name']\n",
    "proj_dir = settings['gcp-bucket-project-dir']\n",
    "result_dir = settings[\"submission-dir\"]\n",
    "\n",
    "fs = gcsfs.GCSFileSystem(project=proj_name)\n",
    "fopen = fs.open(proj_dir + '/spark_data_io.py')\n",
    "exec(fopen.read())\n",
    "fopen.close()\n",
    "fopen = fs.open(proj_dir + '/spark_transform.py')\n",
    "exec(fopen.read())\n",
    "fopen.close()\n",
    "fopen = fs.open(proj_dir + '/spark_processing.py')\n",
    "exec(fopen.read())\n",
    "fopen.close()\n",
    "\n",
    "results = predict_subjects(gs_dir, subjects, sc, num_nodes)\n",
    "if len(results) == 12:\n",
    "    #If using full dataset from all 12 subject folders, generate a submission file\n",
    "    submission_df = generate_pred_result(results, gs_dir, sc).write.csv()\n",
    "    with fs.open('/'.join([proj_dir, result_dir,'submissions.csv']), 'w') as f:   \n",
    "        submission_df = test_submission_df.toPandas()\n",
    "        submission_df['clip'] = submission_df['clip'] + '.mat'\n",
    "        submission_df.to_csv(f, index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
