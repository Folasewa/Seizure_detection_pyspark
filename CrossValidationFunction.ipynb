{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485\n",
      "459\n",
      "497\n",
      "449\n",
      "Fold 0:\n",
      "Accuracy: 0.9876288659793815\n",
      "Average F1 score: 0.9877718919801443\n",
      "Average Precision: 0.9877718919801443\n",
      "Average Recall: 0.9876288659793815\n",
      "Average Roc_auc: 0.9656227180527382\n",
      "Fold 1:\n",
      "Accuracy: 0.9803921568627451\n",
      "Average F1 score: 0.9820100453974693\n",
      "Average Precision: 0.9820100453974693\n",
      "Average Recall: 0.9803921568627452\n",
      "Average Roc_auc: 0.9330144183150865\n",
      "Fold 2:\n",
      "Accuracy: 0.9758551307847082\n",
      "Average F1 score: 0.9768780460284113\n",
      "Average Precision: 0.9768780460284113\n",
      "Average Recall: 0.9758551307847083\n",
      "Average Roc_auc: 0.9178987201248103\n",
      "Fold 3:\n",
      "Accuracy: 0.9933184855233853\n",
      "Average F1 score: 0.9933987921877677\n",
      "Average Precision: 0.9933987921877677\n",
      "Average Recall: 0.9933184855233853\n",
      "Average Roc_auc: 0.9897524680627019\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import json\n",
    "import gcsfs\n",
    "\n",
    "\n",
    "\n",
    "def generateCV(labeled_df, kfolds):\n",
    "    weight = 1 / kfolds\n",
    "    parts = labeled_df.randomSplit( [weight] *  kfolds)\n",
    "\n",
    "    cv_groups = []\n",
    "    \n",
    "    for i in range(kfolds):\n",
    "        print(parts[i].count())\n",
    "        test_df = parts[i]\n",
    "        train_df = sqlContext.createDataFrame(sc.emptyRDD(), labeled_df.schema)\n",
    "        for j in range(kfolds):\n",
    "            if j != i:\n",
    "                train_df = train_df.unionAll(parts[j])\n",
    "        cv_groups.append((train_df, test_df))\n",
    "    return cv_groups\n",
    "\n",
    "\n",
    "\n",
    "def cross_validate(gs_dir, subjects, sc, fs, num_nodes):\n",
    "    '''Train the model with preset params with full scope of labelled data from provided subjects list \n",
    "    and save the trained classfier to google cloud storage'''\n",
    "\n",
    "    json_str_rdd = sc.textFile(gs_dir + '/SETTINGS.json')\n",
    "    json_str = ''.join(json_str_rdd.collect())\n",
    "    settings = json.loads(json_str)\n",
    "    \n",
    "    proj_name = settings['gcp-project-name']\n",
    "    proj_dir = settings['gcp-bucket-project-dir']\n",
    "    dataset_dir = settings['dataset-dir']\n",
    "    fs = gcsfs.GCSFileSystem(project = proj_name)\n",
    "    \n",
    "    \n",
    "    subjects_ave_aucs = []\n",
    "    subjects_cfn_mtxs = []\n",
    "    for subject in subjects:\n",
    "        \n",
    "        #Load data into rdd\n",
    "        start_time = time.time()\n",
    "        loader = dataloader('/'.join([proj_dir,dataset_dir,subject]), fs)\n",
    "        ictal_raw = loader.load_ictal_data()\n",
    "        interictal_raw = loader.load_interictal_data()\n",
    "        partitionNum = num_nodes * 10\n",
    "        ictal_rdd = sc.parallelize(ictal_raw, partitionNum)\n",
    "        interictal_rdd = sc.parallelize(interictal_raw, partitionNum)\n",
    "        end_time = time.time()\n",
    "        #print('--- '+ subject + \": Data Loading %s seconds ---\" % (end_time - start_time))\n",
    "        #Data preprocessing and transformation\n",
    "        start_time = time.time()\n",
    "        transformed_ictal_rdd = ictal_rdd.map(lambda x: process_raw_sample(x, True, sample_transform)).cache()\n",
    "        transformed_interictal_rdd = interictal_rdd.map(lambda x: process_raw_sample(x, False, sample_transform)).cache()\n",
    "\n",
    "        def rddToDf(x):\n",
    "            '''Convert rdd to  and pass this function in Row() args'''\n",
    "            sample_X, sample_y = x\n",
    "            d = {}\n",
    "            d['features'] = Vectors.dense(sample_X)\n",
    "            d['labels'] = sample_y\n",
    "            return d\n",
    "\n",
    "        ictal_df = transformed_ictal_rdd.map(lambda x: Row(**rddToDf(x))).toDF()\n",
    "        interictal_df = transformed_interictal_rdd.map(lambda x: Row(**rddToDf(x))).toDF()\n",
    "        labeled_df = ictal_df.unionAll(interictal_df)\n",
    "        labeled_df.cache()\n",
    "        labeled_df.rdd.count()\n",
    "        end_time = time.time()\n",
    "        #print('--- '+ subject + \": Data Transformation %s seconds ---\" % (end_time - start_time))\n",
    "\n",
    "        #Generate training and test pairs\n",
    "#         rf = set_model(3000, 'labels', seed = 130, maxDepth = 5)\n",
    "#         start_time = time.time()\n",
    "#         model = rf.fit(labeled_df)\n",
    "#         end_time = time.time()\n",
    "#         print('--- '+ subject + \": Model Training %s seconds ---\" % (end_time - start_time))\n",
    "#         print('--- '+ subject + \": Saving Trained Model ---\" )\n",
    "        cv_iterations  = generateCV(labeled_df, 4)\n",
    "        count = 0\n",
    "        cfsn_mtxs = []\n",
    "        ave_aucs = []\n",
    "        for (training_df, test_df) in cv_iterations:\n",
    "            rf = set_model(numTrees = 2500, labelCol = 'labels', seed = 1, maxDepth = 5, subSampling = 0.95)\n",
    "            model = rf.fit(training_df)\n",
    "            result = model.transform(test_df)\n",
    "            print('Fold '+ str(count) + ':')\n",
    "            count += 1\n",
    "            metric = evaluateClassifer(result)\n",
    "            cfsn_mtxs.append(metric.confusionMatrix().toArray())\n",
    "            ave_auc = customEvaluate(result)\n",
    "            print('Average Roc_auc: ' + str(ave_auc))\n",
    "            ave_aucs.append(ave_auc)\n",
    "            del training_df\n",
    "            del test_df\n",
    "        subjects_ave_aucs.append(ave_aucs)\n",
    "        subjects_cfn_mtxs.append(cfsn_mtxs)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    return (subjects_ave_aucs, subjects_cfn_mtxs)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_nodes = 4\n",
    "subjects = ['Patient_8']\n",
    "gs_dir = \"gs://seizure_detection_data/notebooks/seizure_detection_spark_gcp\"\n",
    "\n",
    "\n",
    "appName = 'seizure_detection'\n",
    "conf = SparkConf().setAppName(appName).setMaster('local')\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set(\"spark.executor.instances\", str(2 * num_nodes))\n",
    "        .set('spark.executor.memory', '15G')\n",
    "        .set('spark.driver.memory', '15G')\n",
    "        .set('spark.driver.maxResultSize', '15G'))\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "    \n",
    "\n",
    "json_str_rdd = sc.textFile(gs_dir + '/SETTINGS.json')\n",
    "json_str = ''.join(json_str_rdd.collect())\n",
    "settings = json.loads(json_str)\n",
    "\n",
    "proj_name = settings['gcp-project-name']\n",
    "proj_dir = settings['gcp-bucket-project-dir']\n",
    "\n",
    "fs = gcsfs.GCSFileSystem(project=proj_name)\n",
    "fopen = fs.open(proj_dir + '/spark_data_io.py')\n",
    "exec(fopen.read())\n",
    "fopen.close()\n",
    "fopen = fs.open(proj_dir + '/spark_transform.py')\n",
    "exec(fopen.read())\n",
    "fopen.close()\n",
    "fopen = fs.open(proj_dir + '/spark_processing.py')\n",
    "exec(fopen.read())\n",
    "fopen.close()\n",
    "fopen = fs.open(proj_dir + '/spark_evaluate.py')\n",
    "exec(fopen.read())\n",
    "fopen.close()\n",
    "\n",
    "\n",
    "\n",
    "subjects_ave_aucs, subjects_cfn_mtxs = cross_validate(gs_dir, subjects, sc, fs, num_nodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
